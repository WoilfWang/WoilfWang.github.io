# 预备知识

#### CART回归树

在CART回归树中，假设数据集为$(x_i,y_i)$，$x_i$为D维特征向量，$y_i$为回归目标。在某一次划分中，假设我们选取的划分特征为d，划分节点为a，因此可将数据集划分为两部分，即$M_1=\{x_i|x_{i,d} \le a \}$和$M_2=\{x_i|x_{i,d} > a \}$。在每一轮划分中，我们该怎么选划分特征d以及划分节点a呢？我们可以由以下优化目标
$$
\min_{d,a}\left(\min_{c_1}\sum_{x_i\in M_1}(y_i-c_1)^2 + \min_{c_2}\sum_{x_i\in M_2}(y_i-c_2)^2 \right)
$$
重复n轮，我们可得到若干个不想交的区域($R_1$,$R_2$,...,$R_J$)，即CART树的叶子结点。它采用的是用最终叶子的均值或者中位数来预测输出结果，即
$$
f(x_i)=\sum c_j I(x_i\in R_j)
$$


# GBDT(梯度提升树)

GBDT也是一种boosting集成学习模型，现在我们的想法是：如何构建出若干个回归树并将它们集成起来？假设我们共构建M个树，那么这个问题可以描述为
$$
\begin{aligned}
f_M(x)&=\sum_m^M T(x;\theta_m) \\
f_m(x)&=f_{m-1}(x)+T(x;\theta_m)
\end{aligned}
$$
$\theta_m$即回归树的参数，式(3)即提升树模型的基本描述方法。接下来我们看怎么构建出每一颗树。

假设训练数据$\{(x_1,y_1),..(x_N,y_N)\}$共$N$个样本，损失函数为$L(f(x),y)$，我们要做的就是最小化经验风险 $ \frac{1}{N}\sum_{i=1}^N L(f(x_i),y_i)$

step1 初始化
$$
f_0(x)=\arg \min_c \sum_{i=1}^N L(c,y_i)
$$
在这里，我们得到了第一个分类器，但此时经验风险偏大，接下来我们将依次构建若干个回归树来一步一步减小经验风险。

step2 构建回归树

假设当准备构建第m个回归树时，我们应该以怎样的优化目标去建这个树呢？

我们先考虑一种简单的情况，就是损失函数$L(f_(x),y)=(y-f(x))^2$。构建完第m个回归树后，对于样本$(x_i,y_i)$损失函数可以写成$L(f_m(x_i),y_i)=(y_i-f_{m-1}(x_i)-T(x_i;\theta_m))^2$。令$r_i=y_i-f_{m-1}(x_i)$，$r_i$也就是第m-1步时集成模型的偏差。

刚提到，我们希望每构建一个回归树，经验风险就会小一点，那么我们就可以 以回归目标为$\{r_1,...,r_N\}$来构建第m个回归树，这样模型$f_m(x)=f_{m-1}+T(x;\theta_m)$的期望风险就会减小，*这个方法也就是基本的提升树方法*。

虽然我们以回归目标$\{r_1,...,r_N\}$来构建第m个回归树可以减小期望风险，但是这种情况难以推广，换句话说就是只有我们的损失函数为均方损失或指数时，才可以直接用偏差作为回归目标来构建第m个回归树。但是实际情况下我们的处理不同的问题我们的损失函数各种各样，比如处理分类问题我们可以用对数似然损失函数。如何构建出一个通用的回归目标去生成第m个回归树呢？。接下来我们看看GBDT怎么去解决这个问题。

构建第m个回归树时，N个样本的回归目标为$\{r_1,...r_N\}$，其中$r_i$为
$$
r_i=-\frac{\partial L(f_{m-1}(x_i),y_i)}{\partial f_{m-1}(x_i)}
$$
即用负梯度来代替偏差作为回归目标，为什么用这个作为回归目标，简单来说就是优化效率高，在后面会论证为啥优化效率高。

step3 更新集成模型

建完第m个回归树后，我们可以得到($R_1,...R_J$) $J$个不想交的区域。当$x_i \in R_j$时，那么第m个回归树的输出$c_{mj}$为
$$
c_{mj}=\arg \min_c \sum_{x_i \in R_j}L(f_{m-1}(x_i)+c,y_i)
$$
简单来看就是使落在$R_j$这个区域内所有的样本损失和最小。

接下来我们更新我们的集成模型
$$
f_m(x)=f_{m-1}(x)+\sum_{j=1}^J c_{mj} I(x \in R_j)
$$
重复若干次，我们得到了一个包含M颗回归树的梯度提升树，即
$$
f_M(x)=\sum_{m=1}^M \sum_j c_{mj}I(x\in c_j)
$$

#### 为什么用负梯度作为建树目标？

首先，如果我们要优化这个问题$\min_\theta f(\theta)$，那么我们知道可以基于梯度下降的方法更新$\theta_m$最快，$\theta_{m+1}=\theta_m-\eta \frac{\partial f(\theta_m)}{\theta_m}$。

在GBDT我们的优化目标为最小化经验风险，对于每个样本的优化目标即为 $\min L(f_m(x),y)$。在这个优化目标中，$\theta_m$即为我们当前的集成模型$f_{m}$，$\theta_{m+1}$即为集成模型$f_{m+1}$。代入上述那个基于梯度的更新方法，我们可以得到
$$
f_{m+1}(x)=f_m(x)-\eta \frac{\partial L(f_m(x),y)}{\partial f_m(x)}
$$
即$f_{m+1}(x)-f_m(x)=T_{m+1}(x)=-\eta \frac{\partial L(f_m(x),y)}{\partial f_m(x)}$。很神奇吧，基于梯度下降的方法我们就得到了第m+1颗回归树的回归目标。

当我们处理回归问题时，我们可以令$L(f_m(x),y)=\frac{1}{2}(f_m(x)-y)^2$。对损失函数求负梯度后，我们可以得到第m+1颗树的建树目标$y-f_m(x)$，也就是偏差。在这种情况下，GBDT就与提升树等价了。